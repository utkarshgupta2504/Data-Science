{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- Clubbing features together to give better results\n",
    "- Removing unnecessary features\n",
    "- Eg: abd, ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One method for feature extraction is `Principal Component Analysis (PCA)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially,\n",
    "$$x \\in R^n$$\n",
    "After PCA,\n",
    "$$x \\in R^k$$\n",
    "where,\n",
    "$$k \\leq n$$\n",
    "Hence, **dimension reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of PCA\n",
    "- Data compression\n",
    "  - Reduction of features, hence whole of dataset\n",
    "  - 50 D -> 5 D\n",
    "  - If variance across any axis is too less, that dimension can be ignored\n",
    "  - The idea -> Variance should be retained\n",
    "- Data Visualisation\n",
    "  - As the dimensions increase, it gets impossible to visualise data\n",
    "  - Use PCA for converting to 2D and then visualise\n",
    "- Speeds up computation\n",
    "  - Less iterations\n",
    "  - Also reduces memory load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "- When reducing a dimension, we do it along the projections\n",
    "- To get the maximum variance, we take care of the spread\n",
    "- We calculate the projection error and try to minimise it\n",
    "$$min\\left(\\sum_{i=1}^m(Projection Error^i)^2\\right)$$\n",
    "- Or we can maximise variance\n",
    "- For nD -> kD, we find k vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
