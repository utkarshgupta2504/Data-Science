{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for Multiple Features\n",
    "- $X$ is a $m \\times n$ matrix\n",
    "  - $m$ is the number of datasets\n",
    "  - $n$ is the number of features\n",
    "  - $X_j^i$ denotes the $i^{th}$ example for the $j^{th}$ feature\n",
    "\n",
    "- $Y$ is a $m \\times 1$ vector\n",
    "  - $m$ number of outputs\n",
    "  - $Y^{i} = f(X^i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "- We have to train a model for the n features, such that we assign some weight to all of them and get a nice prediction line to fit as many possible datasets to get an accurate Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "- We have to modify the hypothesis for multiple features to fit\n",
    "$$\\hat{y} = h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n$$\n",
    "- Which is basically a weighted sum where $\\theta_i$ denotes the weight of feature $x_i$\n",
    "$$\\hat{y} = \\sum_{i=0}^n\\theta_ix_i$$\n",
    "> $x_0$ is a dummy feature which is always $1$\n",
    ">\n",
    "> Hence our modified matrix will be $m \\times (n+1)$ with $x_0$ being $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a row vector $\\theta$, we can write\n",
    "$$h_\\theta(x) = \\theta^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified error function\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m[y^{(i)} - \\hat{y}^{(i)}]^2$$\n",
    "Replacing $\\hat{y}$\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m[y^{(i)} - \\theta^Tx^{(i)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating gradient\n",
    "- We have to try to minimise the error, so we modify theta\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta)$$\n",
    "For a generic $\\theta_j$\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}J(\\theta) = \\frac{\\delta}{\\delta\\theta_j}(h_\\theta(x) - y)^2$$\n",
    "$$= (h_\\theta(x) - y)\\frac{\\delta}{\\delta\\theta_j}[\\sum_{j=0}^n\\theta_jx_j]$$\n",
    "All terms cancel as they dont depend on $\\theta_j$, only $x_j$ remains\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}J(\\theta) = (\\hat{y} - y)x_j$$\n",
    "For all examples,\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}J(\\theta) = \\sum_{i=1}^m(\\hat{y}^{(i)} - y^{(i)})x^{(i)}$$\n",
    "Finally,\n",
    "$$\\theta_j = \\theta_j - \\eta \\cdot \\sum_{i=1}^m(\\hat{y}^{(i)} - y^{(i)})x^{(i)}_j$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
