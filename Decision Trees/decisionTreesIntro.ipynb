{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "- Model makes a decision at every node, and then proceeds to that path\n",
    "- A simple tree like structure is formed\n",
    "- Decisions can be easily explained, the structure is right in front of us\n",
    "- Categorical Values are preferred, as the decision process is quite discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buidling a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "- It is defined as the randomness of the data\n",
    "$$H(S) = -\\sum P_c \\cdot log_2 (P_c)$$\n",
    "where, $P_c$ is the probability of class c\n",
    "- Entropy is maximum when the all classes have equal probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How we use entropy?\n",
    "- We iterate over all the features, and divide them in classes\n",
    "- Then we compute the reduction in entropy on division in classes\n",
    "- This reduction is called `Information Gain`\n",
    "- Higher the enformation gain, more preference is given to that feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain\n",
    "- Defined as the reduction in entropy\n",
    "- Assume the original set (S) is divided in S1 and S2, depending on values\n",
    "- Information gain is given by:\n",
    "$$IG(S,A) = H(S) - \\sum\\frac{|S_v|}{|S|}H(S_v)$$\n",
    "where,\n",
    "- $A$ is the attribute\n",
    "- $S_v$ is the count in that set\n",
    "- $H(S)$ is the original entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
