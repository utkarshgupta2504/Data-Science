{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Supervised algorithm\n",
    "- Basically a binary classification, but follows rules of regression\n",
    "- Examples\n",
    "  - Spam / Not spam mails\n",
    "  - Rainy / Sunny Weather\n",
    "  - Cat / Dog image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current regression approach\n",
    "- Apply hypothesis $h_{\\theta}(x)$\n",
    "- We get a real number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need now is to get a value for binary classification, hence we use something called the `sigmoid function`\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "A sigmoid function gives a value 0 or 1, but also respects the distance from the partition line, hence higher surity on mre distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Hypothesis\n",
    "$$h'(\\theta) = g(\\theta^TX)$$\n",
    "$$h'(\\theta) = \\frac{1}{1 + e^{-\\left(\\theta^TX\\right)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For values between 0 and 1, we can set a threshold, if value > threshold, then 1 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- We need a `convex` function to avoid local minimas\n",
    "- Squared error method turns out to be non-convex for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Loss / Binary Cross Entropy\n",
    "- This is the function that we use for loss in logistic regression\n",
    "$$Loss = -\\frac{1}{m}\\sum_{i=1}^m\\left[ y^{(i)} log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)})) \\right]$$\n",
    "- Explanation\n",
    "  - $h_{\\theta}(x^{(i)})$ is the prediction of class 0\n",
    "  - $1 - h_{\\theta}(x^{(i)})$ is the prediction of class 1\n",
    "  - We basically multiply them with the weight of the prediction\n",
    "  - Hence first expression gives us an exponentially decreasing curve\n",
    "  - Second gives us an exponentially increasing curve\n",
    "  - When we add the both, we get a `convex` curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
