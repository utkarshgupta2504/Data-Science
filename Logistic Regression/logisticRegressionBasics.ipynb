{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Supervised algorithm\n",
    "- Basically a binary classification, but follows rules of regression\n",
    "- Examples\n",
    "  - Spam / Not spam mails\n",
    "  - Rainy / Sunny Weather\n",
    "  - Cat / Dog image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current regression approach\n",
    "- Apply hypothesis $h_{\\theta}(x)$\n",
    "- We get a real number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need now is to get a value for binary classification, hence we use something called the `sigmoid function`\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "A sigmoid function gives a value 0 or 1, but also respects the distance from the partition line, hence higher surity on mre distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Hypothesis\n",
    "$$h'(\\theta) = g(\\theta^TX)$$\n",
    "$$h'(\\theta) = \\frac{1}{1 + e^{-\\left(\\theta^TX\\right)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For values between 0 and 1, we can set a threshold, if value > threshold, then 1 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "- We need a `convex` function to avoid local minimas\n",
    "- Squared error method turns out to be non-convex for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Loss / Binary Cross Entropy\n",
    "- This is the function that we use for loss in logistic regression\n",
    "$$Loss = -\\frac{1}{m}\\sum_{i=1}^m\\left[ y^{(i)} log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)})) \\right]$$\n",
    "- Explanation\n",
    "  - $h_{\\theta}(x^{(i)})$ is the prediction of class 0\n",
    "  - $1 - h_{\\theta}(x^{(i)})$ is the prediction of class 1\n",
    "  - We basically multiply them with the weight of the prediction\n",
    "  - Hence first expression gives us an exponentially decreasing curve\n",
    "  - Second gives us an exponentially increasing curve\n",
    "  - When we add the both, we get a `convex` curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m\\left[ y^{(i)} log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)})) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimise this, we put differential as 0,\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}J(\\theta) = -\\sum_{i=1}^m\\left[ \\frac{y^{(i)}}{h_{\\theta}(x^{(i)})} + \\frac{(1 - y^{(i)})}{1 - h_{\\theta}(x^{(i)})} \\right]g'(\\theta^TX^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By property of sigmoid,\n",
    "$$g'(z) = g(z) \\cdot g(1-z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On simplifying,\n",
    "$$\\frac{\\delta}{\\delta\\theta_j}J(\\theta) = -\\sum_{i=1}^m\\left[ y^{(i)} - h_{\\theta}(x^{(i)}) \\right] X_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the update\n",
    "$$ \\theta_j = \\theta_j + \\eta \\cdot \\sum_{i=1}^m \\left[ y^{(i)} - h_{\\theta}(x^{(i)}) \\right] X_j^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the similarity with linear regression, the only change is that the hypothesis is a sigmoid function, hence the name logistic **regression**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
