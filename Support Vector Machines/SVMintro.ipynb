{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "- SVM is a pwerful classifier that can work on both linear and non-linear seperated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working\n",
    "- Finds an optimal hyperplane\n",
    "- Maximises distance between nearest point and itself\n",
    "- Nearest points are called `Support Vectors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplanes\n",
    "- They are a $n-1$ dimension plane in a n feature setup\n",
    "- Seperates 2 classes\n",
    "- If we get a proper hyperplane with max distances, it is called `Maximum Margin Hyperplane`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a matrix $X$ containing $n$ vectors $\\{x_1, x_2, x_3, \\cdots, x_n\\}$\n",
    "- And $Y$ vector having $n$ labels $\\{ y_1, y_2, y_3, \\cdots, y_n \\}$\n",
    "- Where $y^{(i)} \\in \\{ -1, 1 \\}$ (binary classification) [We take -1 & 1 instead of 0 & 1]\n",
    "- A hyper plane is defined as\n",
    "$$w^Tx + b = 0$$\n",
    "Also,\n",
    "$$ \\underbrace{\\theta_0}_b + \\underbrace{\\theta_1x_1^{(i)} + \\theta_2x_2^{(i)} + \\theta_3x_3^{(i)} + \\cdots}_{w^Tx} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Hyperplane\n",
    "- Should seperate data\n",
    "$$w^Tx + b \\begin{cases} \\gt 0 & x^{(i)} \\in \\text{+ve class} \\\\ \\lt 0 & x^{(i)} \\in \\text{-ve class}\\end{cases}$$\n",
    "- Predictions should be confident, the hyperplane should be `maximum margin`\n",
    "  - $\\text{confidence} \\propto \\text{distance}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "$$y_{pred} = g(w^TX + b)$$\n",
    "where,\n",
    "$$g(z) \\begin{cases} +1, & z \\geq 0 \\\\ -1, & z \\lt 0 \\end{cases}$$\n",
    "where,\n",
    "- z is the distance of the point from hyperplane, calculated by\n",
    "$$\\text{dist} = \\frac{w^TX + b}{\\sqrt{w_1^2 + w_2^2 + w_3^2 + \\cdots}} $$\n",
    "$$\\text{dist} = \\frac{w^TX + b}{||w||_2}$$\n",
    "where,\n",
    "- $||w||_2$ is the $L_2$ norm of w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence our Goal becomes\n",
    "**Maximizing the minimum distance of point from the hyperplane**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $i^{th}$ distance,\n",
    "$$\\gamma^{(i)} = \\frac{w^TX^{(i)} + b}{||w||_2}$$\n",
    "Then, minimum distance,\n",
    "$$\\gamma = \\min_{i=1}^m \\gamma^{(i)}$$\n",
    "This means that all the points have at least $\\gamma$ distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Objective\n",
    "$$\\max_{\\gamma,w,b} \\gamma$$ \n",
    "such that \n",
    "$$y^{(i)} \\cdot (w^TX + b) \\geq \\gamma \\text{      } \\forall i \\in \\{ 1, 2, \\cdots, m \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
