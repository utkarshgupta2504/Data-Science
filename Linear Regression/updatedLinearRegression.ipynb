{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression + Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We initialise a hypotheses to get $\\theta =$ init()\n",
    "2. We compute error function by\n",
    "$$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m[\\hat{y}^{(i)} - y^{(i)}]^2$$\n",
    "3. Then we update $\\theta$ using Gradient Descent\n",
    "$$\\theta = \\begin{pmatrix}\\theta_0 \\ \\theta_1\\end{pmatrix}$$\n",
    "$$\\hat{y}^{(i)} = h_{\\theta}(x^{(i)}) = \\theta_0 + \\theta_1x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given, we have 2 parameters, the plot between $J(\\theta), \\theta_0, \\theta_1$ will be a 3D graph, of a `bowl` shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we try to find the minimum error by finding the bottom most part of the bowl, for some $\\theta^*$\n",
    "$$\\theta^* = \\begin{pmatrix} \\theta_0^* \\\\ \\theta_1^* \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the partial derivatives:\n",
    "$$\\frac{\\delta(\\theta)}{\\delta\\theta_0} = \\frac{2}{m}\\sum_{i=1}^m[\\hat{y}^{(i)} - y^{(i)}]$$\n",
    "$$\\frac{\\delta(\\theta)}{\\delta\\theta_1} = \\frac{2}{m}\\sum_{i=1}^m[\\hat{y}^{(i)} - y^{(i)}]x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the update rule will be:\\\n",
    "For some `learning rate` $\\eta$\n",
    "$$\\theta_0 = \\theta_0 - \\eta \\cdot \\frac{1}{m}\\sum_{i=1}^m[\\hat{y}^{(i)} - y^{(i)}]$$\n",
    "$$\\theta_0 = \\theta_0 - \\eta \\cdot \\frac{1}{m}\\sum_{i=1}^m[\\hat{y}^{(i)} - y^{(i)}]x^{(i)}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
