{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "- This is a matrix we make for a binary classification\n",
    "- This helps us in observing metrics of our classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| n = 165 | Predicted : NO| Predicted: YES | |\n",
    "|---|---|---|---|\n",
    "| **Actual : NO** | TN = 50 | FP = 10 | 60 |\n",
    "| **Actual : YES** | FN = 5 | TP = 100 | 105 |\n",
    "|   | 55 | 110 |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    "- TN - True Negatives\n",
    "- FP - False Positives\n",
    "- FN - False Negatives\n",
    "- TP - True Positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 4 metrics can help us visualise and fine tune the classifier as per our needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define more metrics as:\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{\\text{Total Examples}}$$\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Accuracy` - The ratio of correct classifications to the total\n",
    "- `Precision` - Percentage of correct predictions\n",
    "- `Recall` - Ratio of positive predictions to total positive examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-Measure\n",
    "- This is a metric also used for scoring a binary classification\n",
    "$$\\text{F-Measure} = \\text{Harmonic Mean}(\\text{Precision}, \\text{Recall})$$\n",
    "$$\\text{F-Measure} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
